{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/hofmann/nngame/package/src/rlbase/\")\n",
    "\n",
    "import misc\n",
    "import env_blackjack\n",
    "import policy\n",
    "import agent\n",
    "import experiment_mc_everyvisit\n",
    "import experiment_mc_exploringstarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5.1\n",
    "Approximate state-value functions for the blackjack policy that sticks only on 20 or 21, computed by Monte Carlo policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_blackjack.BlackjackEnvironment()\n",
    "pi = policy.DeterministicPolicy(env=env,best_actions={s:[s[0]<20] for s in env.states})\n",
    "ag = agent.BaseAgent(env=env,pi=pi)\n",
    "ex = experiment_mc_everyvisit.MC_EveryVisitExperiment(env=env,agent=ag,n_episodes=500000)\n",
    "ex.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealer-Label\n",
    "dealer_label = [\"A\"]+list(range(2,11))\n",
    "\n",
    "player, dealer = env.get_player_dealer()\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "arr = np.zeros((len(player),len(dealer),2))\n",
    "for (ix,x) in enumerate(dealer):\n",
    "  for (iy,y) in enumerate(player):\n",
    "    for j in range(2):\n",
    "      if x==11:\n",
    "        arr[iy,0,j] = ex.V[(y,x,j)]\n",
    "      else:\n",
    "        arr[iy,ix+1,j] = ex.V[(y,x,j)]        \n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(arr[:,:,0],origin=\"lower\",vmin=-1,vmax=1)\n",
    "plt.xticks(range(len(dealer_label)),dealer_label)\n",
    "plt.xlabel(\"Dealer\")\n",
    "plt.yticks(range(len(player)),player)\n",
    "plt.ylabel(\"Player\")\n",
    "plt.title(\"No usable ace\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(arr[:,:,1],origin=\"lower\",vmin=-1,vmax=1)\n",
    "plt.xticks(range(len(dealer_label)),dealer_label)\n",
    "plt.xlabel(\"Dealer\")\n",
    "plt.yticks(range(len(player)),player)\n",
    "plt.ylabel(\"Player\")\n",
    "plt.title(\"Usable ace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5.3 Solving Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiment_mc_exploringstarts\n",
    "import agent\n",
    "\n",
    "env = env_blackjack.BlackjackEnvironment()\n",
    "pi0 = policy.DeterministicPolicy(env=env,best_actions={s:[s[0]<20] for s in env.states})\n",
    "pi = policy.EpsGreedy(env=env,eps=0.05,det_policy=pi0)\n",
    "agent = agent.BaseAgent(env=env,pi=pi)\n",
    "ex = experiment_mc_exploringstarts.MC_ExploringStartsExperiment(env=env,agent=agent,n_episodes=1e6)\n",
    "ex.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealer-Label\n",
    "dealer_label = [\"A\"]+list(range(2,11))\n",
    "player, dealer = env.get_player_dealer()\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "arr = np.zeros((len(player),len(dealer),2))\n",
    "for (ix,x) in enumerate(dealer):\n",
    "  for (iy,y) in enumerate(player):\n",
    "    for j in range(2):\n",
    "      s = (y,x,j)\n",
    "      if x==11:\n",
    "        arr[iy,0,j] = agent.pi.prob(0,s)>agent.pi.prob(1,s)\n",
    "      else:\n",
    "        arr[iy,ix+1,j] = agent.pi.prob(0,s)>agent.pi.prob(1,s)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(arr[:,:,0],origin=\"lower\",vmin=-1,vmax=1)\n",
    "plt.xticks(range(len(dealer_label)),dealer_label)\n",
    "plt.xlabel(\"Dealer\")\n",
    "plt.yticks(range(len(player)),player)\n",
    "plt.ylabel(\"Player\")\n",
    "plt.title(\"No usable ace\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(arr[:,:,1],origin=\"lower\",vmin=-1,vmax=1)\n",
    "plt.xticks(range(len(dealer_label)),dealer_label)\n",
    "plt.xlabel(\"Dealer\")\n",
    "plt.yticks(range(len(player)),player)\n",
    "plt.ylabel(\"Player\")\n",
    "plt.title(\"Usable ace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5.4 Off-policy Estimation of a Blackjack State Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_blackjack.BlackjackEnvironment()\n",
    "\n",
    "s0 = (13,2,1)\n",
    "n_episodes = int(1e8)\n",
    "G = 0\n",
    "for i in tqdm(range(n_episodes)):\n",
    "    s = s0\n",
    "    while s:\n",
    "        a = (s[0]<20)\n",
    "        r,s,_ = env.step(s,a)\n",
    "    G += r\n",
    "G0 = G / n_episodes\n",
    "print(\"My G0:\",G0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5.3\n",
    "Weighted importance sampling produces lower error estimates of the value of a\n",
    "single blackjack state from off-policy episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G0 = -0.27726 #From book\n",
    "\n",
    "s0 = (13,2,1)\n",
    "\n",
    "# Now use behavorial policy\n",
    "def b(s):\n",
    "  return np.random.rand()<.5\n",
    "\n",
    "n_trials = 500\n",
    "n_episodes = int(1e4)\n",
    "ordinary = np.zeros((n_trials,n_episodes))\n",
    "weighted = np.zeros((n_trials,n_episodes))\n",
    "for trial in tqdm(range(n_trials)):\n",
    "  my_sum = 0\n",
    "  my_sum2 = 0\n",
    "  for i in range(n_episodes):\n",
    "    s = s0\n",
    "    episode = []\n",
    "    rho = 1\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "      a = b(s)\n",
    "      if a:\n",
    "        rho *= (s[0]<20) / (1/2) # Target policy prob. divided by behavorial policy prob.\n",
    "      else: \n",
    "        rho *= (s[0]>19) / (1/2) # Target policy prob. divided by behavorial policy prob.\n",
    "      r,s,terminal = env.step(s,a)\n",
    "    my_sum += rho*r\n",
    "    my_sum2 += rho\n",
    "    ordinary[trial,i] = my_sum / (i+1)\n",
    "    if my_sum2 > 0:\n",
    "      weighted[trial,i] = my_sum / my_sum2\n",
    "ordinary_mse = np.sum((ordinary-G0)**2,axis=0)/n_trials\n",
    "weighted_mse = np.sum((weighted-G0)**2,axis=0)/n_trials\n",
    "plt.semilogx(ordinary_mse,\"g\")\n",
    "plt.semilogx(weighted_mse,\"r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5.4 \n",
    "\n",
    "Ordinary importance sampling produces surprisingly unstable estimates on the\n",
    "one-state MDP shown inset (Example 5.5). The correct estimate here is 1 ($\\gamma$ = 1), and, even\n",
    "though this is the expected value of a sample return (after importance sampling), the variance\n",
    "of the samples is infinite, and the estimates do not converge to this value. These results are for\n",
    "off-policy first-visit MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_trials = 10\n",
    "n_episodes = int(1e8)\n",
    "ordinary = np.zeros((n_episodes,n_trials))\n",
    "for trial in tqdm(range(n_trials)):\n",
    "  my_sum = 0\n",
    "  n_states = 0\n",
    "  n_visits = 1\n",
    "  for i in tqdm(range(n_episodes),leave=False):\n",
    "    rhor = 1\n",
    "    while True:\n",
    "      if np.random.rand()<.5:\n",
    "        rhor = 0\n",
    "        break\n",
    "      rhor *= 2\n",
    "      if np.random.rand()>0.9:\n",
    "        break   \n",
    "      n_visits += 1\n",
    "    my_sum += rhor\n",
    "    ordinary[i,trial] = my_sum / n_visits\n",
    "plt.semilogx(range(1,n_episodes+1),ordinary)\n",
    "plt.ylim(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env_racetrack\n",
    "import experiment_mc_offpolicy\n",
    "from importlib import reload\n",
    "reload(env_racetrack)\n",
    "reload(experiment_mc_offpolicy)\n",
    "reload(policy)\n",
    "\n",
    "def plot(i, episode):  \n",
    "  global lens\n",
    "  lens += [len(episode)]\n",
    "  if i%50==0:\n",
    "    x = [x for ((x,_,_,_),_,_) in episode]\n",
    "    y = [y for ((_,y,_,_),_,_) in episode]\n",
    "  \n",
    "    ax1.clear()\n",
    "    ax1.imshow(np.swapaxes(env.field,0,1),origin=\"lower\")\n",
    "    ax1.plot(x,y,'rx-')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.imshow(np.swapaxes(env.field,0,1),origin=\"lower\")\n",
    "    arr_u = np.zeros((env.sx,env.sy))\n",
    "    arr_v = np.zeros((env.sx,env.sy))\n",
    "    for s in env.states:\n",
    "      ax,ay = ag.pi.get(s)\n",
    "      x,y,_,_ = s\n",
    "      arr_u[x,y] += ax\n",
    "      arr_v[x,y] += ay\n",
    "    for x in range(env.sx):\n",
    "      for y in range(env.sy):\n",
    "        v = (arr_u[x,y]**2 + arr_v[x,y]**2)**.5\n",
    "        if v>0:\n",
    "          arr_u[x,y] /= v\n",
    "          arr_v[x,y] /= v\n",
    "    ax2.quiver(np.swapaxes(arr_u,0,1),np.swapaxes(arr_v,0,1),units='xy',angles='xy',scale=1,\n",
    "               width=0.1,headwidth=6)\n",
    "    \n",
    "    ax3.clear()\n",
    "    ax3.semilogy(lens,'.')\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "k = 4\n",
    "ax1 = plt.subplot(k,2,(1,2*k-3))\n",
    "ax2 = plt.subplot(k,2,(2,2*k-2))\n",
    "ax3 = plt.subplot(k,2,(2*k-1,2*k))\n",
    "\n",
    "env = env_racetrack.RacetrackEnvironment()\n",
    "pi = policy.DeterministicPolicy(env=env,best_actions={s:[(0,0)] for s in env.states})\n",
    "ag = agent.BaseAgent(env=env,pi=pi)\n",
    "exp = experiment_mc_offpolicy.MC_OffPolicyExperiment(env=env,agent=ag,n_episodes=1e5,q_init=-1e6,eps=0.01,callback=plot)\n",
    "lens = []\n",
    "\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "exp.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
