{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'misc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-950c124d13ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrlbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menv_bandit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_simple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rlbase/policy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform_Q_to_BestAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'misc'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rlbase import env_bandit, policy, misc, agent_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"/home/hofmann/nngame/package/src/rlbase/\")\n",
    "\n",
    "#import misc\n",
    "#import env_bandit\n",
    "#import policy\n",
    "#import agent_simple\n",
    "#import agent_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, optimal_actions):\n",
    "    plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.subplot(2,1,1)\n",
    "    for k in results.keys():\n",
    "      plt.plot(results[k])\n",
    "    plt.legend(results.keys())\n",
    "    plt.subplot(2,1,2)\n",
    "    for k in optimal_actions.keys():\n",
    "      plt.plot(optimal_actions[k])\n",
    "    plt.legend(optimal_actions.keys())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.2\n",
    "Average performance of $\\epsilon$-greedy action-value methods on the 10-armed testbed. These data are averages over 2000 runs with different bandit problems. All methods used sample averages as their action-value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_timesteps = 1000\n",
    "N_bandits = 2000\n",
    "\n",
    "agent_infos = {\"$\\epsilon$=0\":{\"eps\":0},\n",
    "               \"$\\epsilon$=0.01\":{\"eps\":0.01},\n",
    "               \"$\\epsilon$=0.1\":{\"eps\":0.1}}\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in agent_infos}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in agent_infos}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  env = rlbase.env_bandit.BanditEnvironment()\n",
    "  for (k,v) in agent_infos.items():\n",
    "    pi = policy.EpsGreedy(env=env, **v)\n",
    "    agent = agent_simple.SimpleAgent(env=env, pi=pi)  \n",
    "    \n",
    "    best_action = list(env.arms).index(max(env.arms))\n",
    "    s = env.start()\n",
    "    a = agent.start(s)\n",
    "    for t in range(N_timesteps):\n",
    "      r, s, _ = env.step(s,a)\n",
    "      a = agent.step(r,s)\n",
    "      results[k][t] += r / N_bandits\n",
    "      optimal_actions[k][t] += (a==best_action) / N_bandits\n",
    "plot_results(results, optimal_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.5\n",
    "Demonstrate the difficulties that sample-average methods have for nonstationary problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_timesteps = 10000\n",
    "N_bandits = 1000\n",
    "\n",
    "agent_infos = {\"SampleAverage\":{\"eps\":0.1},\n",
    "               \"ConstantStepsize\":{\"eps\":0.1,\"alpha\":0.1}}\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in agent_infos}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in agent_infos}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  env = env_bandit.BanditEnvironment(random=True)\n",
    "  for (k,v) in agent_infos.items():\n",
    "    pi = policy.EpsGreedy(env=env, **v)\n",
    "    agent = agent_simple.SimpleAgent(env=env,pi=pi,**v)\n",
    "    s = env.start()\n",
    "    a = agent.start(s)\n",
    "    for t in range(N_timesteps):\n",
    "      best_action = list(env.arms).index(max(env.arms))\n",
    "      r, s, _ = env.step(s,a)\n",
    "      a = agent.step(r,s)\n",
    "      results[k][t] += r / N_bandits\n",
    "      optimal_actions[k][t] += (a==best_action) / N_bandits\n",
    "plot_results(results, optimal_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.3\n",
    "The effect of optimistic initial action-value estimates on the 10-armed testbed. Both methods use a constant step-size parameter, $\\alpha=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_timesteps = 1000\n",
    "N_bandits = 1000\n",
    "\n",
    "agent_infos = {\"$\\epsilon$=0.1 Q=0\":{\"eps\":0.1},\n",
    "               \"$\\epsilon$=0  Q=5\":{\"eps\":0,\"q_init\":5}}\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in agent_infos}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in agent_infos}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  env = env_bandit.BanditEnvironment()\n",
    "  for (k,v) in agent_infos.items():\n",
    "    pi = policy.EpsGreedy(env=env,eps=0.1)\n",
    "    agent = agent_simple.SimpleAgent(env=env,pi=pi,alpha=0.1,**v)\n",
    "    s = env.start()\n",
    "    a = agent.start(s)\n",
    "    best_action = list(env.arms).index(max(env.arms))\n",
    "    for t in range(N_timesteps):\n",
    "      r, s, _ = env.step(s,a)\n",
    "      a = agent.step(r,s)\n",
    "      results[k][t] += r / N_bandits\n",
    "      optimal_actions[k][t] += (a==best_action) / N_bandits\n",
    "plot_results(results, optimal_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.4\n",
    "Average performance of UCB action selection on the 10-armed testbed. As shown, UCB generally performs better than $\\epsilon$-greedy action selection, except in the first $k$ steps, when it selects randomly among the as-yet-untried actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_timesteps = 1000\n",
    "N_bandits = 1000\n",
    "\n",
    "agent_infos = {\"$\\epsilon$=0.1\":{\"eps\":0.1},\n",
    "               \"UCB c=2\":{\"eps\":0,\"ucb_c\":2}}\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in agent_infos}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in agent_infos}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  env = env_bandit.BanditEnvironment()\n",
    "  for (k,v) in agent_infos.items():\n",
    "    pi = policy.EpsGreedy(env=env,**v)\n",
    "    agent = agent_simple.SimpleAgent(env=env,pi=pi,**v)\n",
    "    s = env.start()\n",
    "    a = agent.start(s)\n",
    "    best_action = list(env.arms).index(max(env.arms))\n",
    "    for t in range(N_timesteps):\n",
    "      r, s, _ = env.step(s,a)\n",
    "      a = agent.step(r,s)\n",
    "      results[k][t] += r / N_bandits\n",
    "      optimal_actions[k][t] += (a==best_action) / N_bandits\n",
    "plot_results(results, optimal_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.5\n",
    "Average performance of the gradient bandit algorithm with and without a reward baseline on the 10-armed testbed when the $q_*(a)$ are chosen to be near $+4$ rather than near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_timesteps = 1000\n",
    "N_bandits = 1000\n",
    "\n",
    "agent_infos = {r\"Without baseline,$\\alpha$=0.1\":{\"baseline\":False,\"alpha\":0.1},\n",
    "               r\"With baseline,$\\alpha$=0.1\":{\"baseline\":True,\"alpha\":0.1},\n",
    "               r\"Without baseline,$\\alpha$=0.4\":{\"baseline\":False,\"alpha\":0.4},\n",
    "               r\"With baseline,$\\alpha$=0.4\":{\"baseline\":True,\"alpha\":0.4}}\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in agent_infos}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in agent_infos}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  env = env_bandit.BanditEnvironment(offset=4)\n",
    "  for (k,v) in agent_infos.items():\n",
    "    pi = policy.Softmax(env=env)\n",
    "    agent = agent_gradient.GradientAgent(env=env,pi=pi,**v)\n",
    "    s = env.start()\n",
    "    a = agent.start(s)\n",
    "    best_action = list(env.arms).index(max(env.arms))\n",
    "    for t in range(N_timesteps):\n",
    "      r, s, _ = env.step(s,a)\n",
    "      a = agent.step(r,s)\n",
    "      results[k][t] += r / N_bandits\n",
    "      optimal_actions[k][t] += (a==best_action) / N_bandits\n",
    "plot_results(results, optimal_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.6\n",
    "A parameter study of the various bandit algorithms presented in this chapter. Each point is the average reward obtained over 1000 steps with a particular algorithm at a particular setting of its parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_timesteps = 1000\n",
    "N_bandits = 1000\n",
    "\n",
    "agent_infos = {}\n",
    "for eps in np.power(2.,np.arange(-7,-1,.5)):\n",
    "    key = (\"$\\epsilon$-greedy\",eps)\n",
    "    value = {\"eps\": eps}\n",
    "    agent_infos[key] = value\n",
    "for ucb_c in np.power(2.,np.arange(-4,3,.5)):\n",
    "    key = (\"UCB\",ucb_c)\n",
    "    value = {\"eps\": 0, \"ucb_c\": ucb_c}\n",
    "    agent_infos[key] = value\n",
    "for val in np.power(2.,np.arange(-2,3,.5)):\n",
    "    key = (\"Optimistic\",val)\n",
    "    value = {\"eps\": 0, \"q_init\": val,\"alpha\":0.1}\n",
    "    agent_infos[key] = value\n",
    "for alpha in np.power(2.,np.arange(-5,3,.5)):\n",
    "    key = (\"Gradient\",alpha)\n",
    "    value = {\"alpha\": alpha}\n",
    "    agent_infos[key] = value\n",
    "     \n",
    "results         = {i:0 for i in agent_infos.keys()}\n",
    "optimal_actions = {i:0 for i in agent_infos.keys()}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  env = env_bandit.BanditEnvironment()\n",
    "  for (k,v) in agent_infos.items():\n",
    "    if k[0]==\"Gradient\":\n",
    "        pi = policy.Softmax(env=env)\n",
    "        agent = agent_gradient.GradientAgent(env=env,pi=pi,**v)\n",
    "    else:\n",
    "        pi = policy.EpsGreedy(env=env,**v)\n",
    "        agent = agent_simple.SimpleAgent(env=env,pi=pi,**v)       \n",
    "    s = env.start()\n",
    "    a = agent.start(s)\n",
    "    best_action = list(env.arms).index(max(env.arms))\n",
    "    for t in range(N_timesteps):\n",
    "      r, s, _ = env.step(s,a)\n",
    "      a = agent.step(r,s)\n",
    "      results[k] += r\n",
    "      optimal_actions[k] += (a==best_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "values_algo = [a for (a,b) in results.keys()]\n",
    "values_x    = [b for (a,b) in results.keys()]\n",
    "values_y    = [v/bandit/N_timesteps for v in results.values()]\n",
    "df = pd.DataFrame({\"x\":values_x,\"y\":values_y,\"algo\":values_algo})\n",
    "\n",
    "cols = {\"$\\epsilon$-greedy\":\"r\",\"UCB\":\"b\",\"Optimistic\":\"k\",\"Gradient\":\"g\"}\n",
    "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "for a in list(set(df.algo)):\n",
    "    d = df[df[\"algo\"]==a]\n",
    "    plt.semilogx(d.x,d.y,cols[a])\n",
    "plt.legend(list(set(df.algo)))\n",
    "\n",
    "exps = np.arange(-7,3)\n",
    "xticks = np.power(2.,exps)\n",
    "labels = [r\"$2^{\"+str(e)+\"}$\" for e in exps]\n",
    "\n",
    "plt.xticks(xticks,labels)\n",
    "plt.ylim(1,1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
