{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.signal\n",
    "import IPython.display\n",
    "\n",
    "from rlbase import misc\n",
    "from rlbase.environment import RandomWalkEnvironment, WindyGridworldEnvironment, CliffGridworldEnvironment,\\\n",
    "Ex67Environment\n",
    "from rlbase.policy import DeterministicPolicy, EpsGreedy\n",
    "from rlbase.agent import SarsaAgent, QlearningAgent, ExpectedSarsaAgent, DoubleQlearningAgent\n",
    "from rlbase.experiment import TD_CtrlExperiment\n",
    "from rlbase.policy_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s0 = 3\n",
    "N = 2*s0-1\n",
    "\n",
    "env = RandomWalkEnvironment(nmax=N)\n",
    "pi = DeterministicPolicy(env=env)\n",
    "\n",
    "v_true = evaluate_policy_linear_system(env,pi)\n",
    "v_true = [v for v in v_true.values()]\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "runs = 100\n",
    "n_episodes = 100\n",
    "\n",
    "legends = []\n",
    "for alpha in tqdm([0.15,0.1,0.05,0.04,0.03,0.02,0.01]):\n",
    "  e_mc = np.zeros((n_episodes,1))\n",
    "  e_td = np.zeros((n_episodes,1))\n",
    "  for run in tqdm(range(runs),leave=False):\n",
    "    v_init = np.ones((N+2,1))*0.5\n",
    "    v_mc = v_init.copy()\n",
    "    v_td = v_init.copy()\n",
    "        \n",
    "    for i in range(n_episodes):\n",
    "      s = s0\n",
    "      episode = []\n",
    "      while True:\n",
    "        r, s_prime, terminal = env.step(s,0)\n",
    "        episode += [(s,r)]\n",
    "        if terminal:\n",
    "          v_td[s] += alpha * (r - v_td[s] )\n",
    "          break\n",
    "        else:\n",
    "          v_td[s] += alpha * (r + gamma * v_td[s_prime] - v_td[s])\n",
    "        s = s_prime\n",
    "      G = 0\n",
    "      for (s,r) in episode[::-1]:\n",
    "        G = gamma * G + r\n",
    "        v_mc[s] += alpha * ( G - v_mc[s] )  \n",
    "      e_mc[i] += (np.sum((v_mc[1:N+1]-v_true[1:N+1])**2))**.5/runs/5**.5\n",
    "      e_td[i] += (np.sum((v_td[1:N+1]-v_true[1:N+1])**2))**.5/runs/5**.5\n",
    "  if alpha>=0.05:\n",
    "    plt.plot(e_td)\n",
    "    legends += [\"TD \"+str(alpha)]\n",
    "  else:\n",
    "    plt.plot(e_mc)\n",
    "    legends += [\"MC \"+str(alpha)]\n",
    "plt.legend(tuple(legends))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "lens = []\n",
    "timesteps = [0]\n",
    "eee = []\n",
    "fig, _ = plt.subplots(6,2,figsize=(5,7))\n",
    "def plot(i,episode):\n",
    "  global lens, timesteps\n",
    "  lens += [len(episode)]\n",
    "  timesteps += [timesteps[-1]+len(episode)]\n",
    "  if i%100==0:\n",
    "    plt.clf()\n",
    "    x = [a[0][0] for a in episode] + [env.terminal_states[0][0]]\n",
    "    y = [a[0][1] for a in episode] + [env.terminal_states[0][1]]\n",
    "    \n",
    "    for idx, di in enumerate([\"up\",\"down\",\"left\",\"right\"]):\n",
    "        plt.subplot(6,2,idx+1)\n",
    "        r = {s: agent.q[s][di] for s in env.states}\n",
    "        plt.imshow(np.transpose(np.flip(env.reshape(r),axis=1)),origin=\"lower\")\n",
    "        plt.colorbar()\n",
    "        plt.title(di)\n",
    "    best_action = {k:misc.argmax_unique(v) for (k,v) in agent.q.items()}\n",
    "    \n",
    "    arrow_length = .3\n",
    "    \n",
    "    plt.subplot(6,2,(5,8))\n",
    "    for j in range(env.sx+1):\n",
    "      plt.plot([j-.5,j-.5],[-.5,env.sy-.5],'k-')\n",
    "    for j in range(env.sy+1):\n",
    "      plt.plot([-.5,env.sx-.5],[j-.5,j-.5],'k-')\n",
    "    for (k,v) in best_action.items():\n",
    "      dx, dy = (0,0)\n",
    "      if \"right\" in v:\n",
    "        dx += arrow_length\n",
    "      if \"up\" in v:\n",
    "        dy += arrow_length\n",
    "      if \"left\" in v:\n",
    "        dx -= arrow_length\n",
    "      if \"down\" in v:\n",
    "        dy -= arrow_length\n",
    "      plt.arrow(k[0],k[1],dx,dy,color=\"r\",head_width=0.1)\n",
    "    for xx in [3,4,5,5.95,6.05,6.95,7.05,8]:\n",
    "      plt.plot([xx,xx],[0,6],dx,dy,color=\"b\")\n",
    "    plt.xlim(-.5,env.sx-.5)\n",
    "    plt.ylim(-.5,env.sy-.5)\n",
    "    plt.plot(env.start[0],env.start[1],\"ob\",markersize=12)\n",
    "    plt.plot(env.terminal_states[0][0],env.terminal_states[0][1],\"og\",markersize=12)\n",
    "    \n",
    "    plt.subplot(6,2,(9,12))\n",
    "    plt.plot(x,y,'-xk')\n",
    "    for j in range(env.sx+1):\n",
    "      plt.plot([j-.5,j-.5],[-.5,env.sy-.5],'k-')\n",
    "    for j in range(env.sy+1):\n",
    "      plt.plot([-.5,env.sx-.5],[j-.5,j-.5],'k-')\n",
    "    for a in episode:\n",
    "      dx, dy = (0,0)\n",
    "      if \"right\" in a[1]:\n",
    "        dx += arrow_length\n",
    "      if \"up\" in a[1]:\n",
    "        dy += arrow_length\n",
    "      if \"left\" in a[1]:\n",
    "        dx -= arrow_length\n",
    "      if \"down\" in a[1]:\n",
    "        dy -= arrow_length\n",
    "      plt.arrow(a[0][0],a[0][1],dx,dy,color=\"r\",head_width=0.1)\n",
    "    for xx in [3,4,5,5.95,6.05,6.95,7.05,8]:\n",
    "      plt.plot([xx,xx],[0,6],dx,dy,color=\"b\")\n",
    "    plt.xlim(-.5,env.sx-.5)\n",
    "    plt.ylim(-.5,env.sy-.5)\n",
    "    plt.plot(env.start[0],env.start[1],\"ob\",markersize=12)\n",
    "    plt.plot(env.terminal_states[0][0],env.terminal_states[0][1],\"og\",markersize=12)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "\n",
    "env = WindyGridworldEnvironment(diagonal=False,stay=False)\n",
    "pi = EpsGreedy(env=env,eps=0.1)\n",
    "agent = SarsaAgent(env=env,pi=pi)\n",
    "exp = TD_CtrlExperiment(env=env,agent=agent,n_episodes=1000,callback=plot)\n",
    "exp.train()\n",
    "plt.figure()\n",
    "plt.plot(timesteps,list(range(len(timesteps))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "rewards = []\n",
    "\n",
    "def count(i,episode):\n",
    "  global rewards\n",
    "  rewards += [sum(r for s,a,r in episode)]\n",
    "    \n",
    "env = CliffGridworldEnvironment()\n",
    "pi = EpsGreedy(env=env,eps=0.1)\n",
    "all_rewards = {\"Qlearning\":np.zeros(500),\"Sarsa\":np.zeros(500)}\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    lens = []\n",
    "    timesteps = [0]\n",
    "    rewards = []\n",
    "    agent = QlearningAgent(pi=pi,env=env)\n",
    "    exp = TD_CtrlExperiment(env=env,agent=agent,n_episodes=500,callback=count,show_progress=False)\n",
    "    exp.train()\n",
    "    all_rewards[\"Qlearning\"] += rewards\n",
    "\n",
    "    lens = []\n",
    "    timesteps = [0]\n",
    "    rewards = []\n",
    "    agent = SarsaAgent(pi=pi,env=env)\n",
    "    exp = TD_CtrlExperiment(env=env,agent=agent,n_episodes=500,callback=count,show_progress=False)\n",
    "    exp.train()\n",
    "    all_rewards[\"Sarsa\"] += rewards\n",
    "    \n",
    "plt.plot(scipy.signal.savgol_filter(all_rewards[\"Qlearning\"]/100,window_length=11,polyorder=1))\n",
    "plt.plot(scipy.signal.savgol_filter(all_rewards[\"Sarsa\"    ]/100,window_length=11,polyorder=1))\n",
    "plt.ylim([-100,-25])\n",
    "plt.legend(all_rewards.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "alphas = [0.1+i*0.05 for i in range(19)]\n",
    "\n",
    "algos = [\"Qlearning\",\"Sarsa\",\"ExpectedSarsa\"]\n",
    "typs = [\"interim\",\"asymptotic\"]\n",
    "\n",
    "runs     = {\"interim\": 50000, \"asymptotic\": 10}\n",
    "episodes = {\"interim\": 100, \"asymptotic\": 100000}\n",
    "\n",
    "runs     = {\"interim\": 100, \"asymptotic\": 10}\n",
    "episodes = {\"interim\": 100, \"asymptotic\": 10000}\n",
    "\n",
    "rewards = 0\n",
    "def count(i,episode):\n",
    "  global rewards\n",
    "  rewards += sum(r for s,a,r in episode)\n",
    "\n",
    "combs = [(typ,algo,alpha) for typ in typs for algo in algos for alpha in alphas]\n",
    "    \n",
    "all_rewards = {comb:0 for comb in combs}\n",
    "env = CliffGridworldEnvironment()\n",
    "for (typ,algo,alpha) in tqdm(combs):\n",
    "    pi = EpsGreedy(env=env,eps=0.1)\n",
    "    for i in range(runs[typ]):\n",
    "        rewards = 0\n",
    "        if algo==\"Qlearning\":\n",
    "            agent = QlearningAgent(pi=pi,env=env,alpha=alpha)\n",
    "        elif algo==\"Sarsa\":\n",
    "            agent = SarsaAgent(pi=pi,env=env,alpha=alpha)\n",
    "        elif algo==\"ExpectedSarsa\":\n",
    "            agent = ExpectedSarsaAgent(pi=pi,env=env,alpha=alpha)\n",
    "        exp = TD_CtrlExperiment(env=env,agent=agent,n_episodes=episodes[typ],callback=count,show_progress=False)\n",
    "        exp.train()\n",
    "        all_rewards[(typ,algo,alpha)] += rewards / episodes[typ] / runs[typ]\n",
    "\n",
    "col ={\"Sarsa\":\"b\",\"Qlearning\":\"k\",\"ExpectedSarsa\":\"r\"}\n",
    "lty = {\"interim\":\"--\",\"asymptotic\":\"-\"}\n",
    "\n",
    "d ={(typ,algo):[] for typ in typs for algo in algos}\n",
    "for (typ,algo,alpha) in combs:\n",
    "    d[(typ,algo)] += [all_rewards[(typ,algo,alpha)]]\n",
    "for typ in typs:\n",
    "    for algo in algos:\n",
    "        plt.plot(alphas,d[(typ,algo)],col[algo]+lty[typ]+\"x\")\n",
    "plt.ylim([-160,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "n_episodes = 500\n",
    "n_runs = 10000\n",
    "\n",
    "def save_data(i,episode):\n",
    "  global tmp\n",
    "  if episode[0][1]==\"l\":\n",
    "    tmp[i] += 1\n",
    "\n",
    "all_mid = {}\n",
    "for algo in tqdm([\"Q\",\"DoubleQ\"]):\n",
    "    tmp = np.zeros(n_episodes)\n",
    "    for i in tqdm(range(n_runs)):\n",
    "        env = Ex67Environment(n=10)\n",
    "        pi = EpsGreedy(env=env,eps=0.1)\n",
    "        if algo==\"Q\":\n",
    "            agent = QlearningAgent(env=env,pi=pi,alpha=0.1)\n",
    "        else:\n",
    "            agent = DoubleQlearningAgent(env=env,pi=pi,alpha=0.1)\n",
    "        exp = TD_CtrlExperiment(env=env,agent=agent,n_episodes=n_episodes,callback=save_data,show_progress=False)\n",
    "        exp.train()\n",
    "    all_mid[algo] = tmp / n_runs\n",
    "\n",
    "plt.figure()\n",
    "for algo in tqdm([\"Q\",\"DoubleQ\"]):\n",
    "    plt.plot(all_mid[algo],\"-\")\n",
    "plt.plot([0,n_episodes],[0.05,0.05],\"--\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
