{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy.random as npr\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/hofmann/nngame/package/src/rlbase/\")\n",
    "from importlib import reload\n",
    "\n",
    "import env_bandit\n",
    "import agent_greedy\n",
    "import agent_gradient\n",
    "import misc\n",
    "from rl_glue import RLGlue\n",
    "reload(env_bandit)\n",
    "reload(agent_greedy)\n",
    "reload(agent_gradient)\n",
    "reload(misc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.2\n",
    "Average performance of $\\epsilon$-greedy action-value methods on the 10-armed testbed. These data are averages over 2000 runs with different bandit problems. All methods used sample averages as their action-value estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "N_timesteps = 1000\n",
    "N_bandits = 2000\n",
    "\n",
    "arr_eps = [0,0.01,0.1]\n",
    "\n",
    "env = env_bandit.BanditEnvironment    \n",
    "env_info = {\"N\": k}\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in arr_eps}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in arr_eps}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  for eps in arr_eps:\n",
    "\n",
    "    agent = agent_greedy.EpsGreedyAgent\n",
    "    agent_info = {\"num_actions\": k, \"epsilon\": eps}    \n",
    "    \n",
    "    rl_glue = RLGlue(env, agent)          \n",
    "    rl_glue.rl_init(agent_info, env_info) \n",
    "    rl_glue.rl_start()                    \n",
    "\n",
    "    best_action = list(rl_glue.environment.arms).index(max(rl_glue.environment.arms))\n",
    "    for t in range(N_timesteps):\n",
    "      reward, _, action, _ = rl_glue.rl_step()\n",
    "      results[eps][t] += reward / N_bandits\n",
    "      optimal_actions[eps][t] += (action==best_action) / N_bandits\n",
    "    \n",
    "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,1)\n",
    "for e in arr_eps:\n",
    "  plt.plot(results[e])\n",
    "plt.legend([\"eps=\"+str(e) for e in arr_eps])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for e in arr_eps:\n",
    "  plt.plot(optimal_actions[e])\n",
    "plt.legend([\"eps=\"+str(e) for e in arr_eps])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.5\n",
    "Demonstrate the difficulties that sample-average methods have for nonstationary problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "N_timesteps = 10000\n",
    "N_bandits = 1000\n",
    "\n",
    "env = env_bandit.BanditEnvironment    \n",
    "env_info = {\"N\": k, \"random\":True}\n",
    "\n",
    "stepsizes = [\"SampleAverage\", \"ConstantStepsize\"]\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in stepsizes}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in stepsizes}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  for s in stepsizes:\n",
    "    agent = agent_greedy.EpsGreedyAgent\n",
    "    if s == \"SampleAverage\":\n",
    "        agent_info = {\"num_actions\": k, \"epsilon\": 0.1}    \n",
    "    else:\n",
    "        agent_info = {\"num_actions\": k, \"epsilon\": 0.1, \"step_size\": 0.1}    \n",
    "    rl_glue = RLGlue(env, agent)          \n",
    "    rl_glue.rl_init(agent_info, env_info) \n",
    "    rl_glue.rl_start()                    \n",
    "\n",
    "    for t in range(N_timesteps):\n",
    "      best_action = list(rl_glue.environment.arms).index(max(rl_glue.environment.arms))\n",
    "      reward, _, action, _ = rl_glue.rl_step()\n",
    "      results[s][t] += reward / N_bandits\n",
    "      optimal_actions[s][t] += (action==best_action) / N_bandits   \n",
    "        \n",
    "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,1)\n",
    "for e in stepsizes:\n",
    "  plt.plot(results[e])\n",
    "plt.legend(stepsizes)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for e in stepsizes:\n",
    "  plt.plot(optimal_actions[e])\n",
    "plt.legend(stepsizes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.3\n",
    "The effect of optimistic initial action-value estimates on the 10-armed testbed. Both methods use a constant step-size parameter, $\\alpha=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "N_timesteps = 1000\n",
    "N_bandits = 1000\n",
    "\n",
    "agent_info_1 = {\"num_actions\": k, \"epsilon\": 0.1, \"step_size\":0.1, \"initial_value\": 0}\n",
    "agent_info_2 = {\"num_actions\": k, \"epsilon\": 0  , \"step_size\":0.1, \"initial_value\": 5}\n",
    "\n",
    "agent_infos = {\"Q=\"+str(e[\"initial_value\"])+\",eps=\"+str(e[\"step_size\"]):e for e in [agent_info_1, agent_info_2]}\n",
    "\n",
    "env = env_bandit.BanditEnvironment    \n",
    "env_info = {\"N\": k}\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in agent_infos}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in agent_infos}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  for k,agent_info in agent_infos.items():\n",
    "\n",
    "    agent = agent_greedy.EpsGreedyAgent\n",
    "    \n",
    "    rl_glue = RLGlue(env, agent)          \n",
    "    rl_glue.rl_init(agent_info, env_info) \n",
    "    rl_glue.rl_start()                    \n",
    "\n",
    "    best_action = list(rl_glue.environment.arms).index(max(rl_glue.environment.arms))\n",
    "    for t in range(N_timesteps):\n",
    "      reward, _, action, _ = rl_glue.rl_step()\n",
    "      results[k][t] += reward / N_bandits\n",
    "      optimal_actions[k][t] += (action==best_action) / N_bandits\n",
    "    \n",
    "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,1)\n",
    "for agent_info in agent_infos:\n",
    "  plt.plot(results[agent_info])\n",
    "plt.legend(agent_infos.keys())\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for agent_info in agent_infos:\n",
    "  plt.plot(optimal_actions[agent_info])\n",
    "plt.legend(agent_infos.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.4\n",
    "Average performance of UCB action selection on the 10-armed testbed. As shown, UCB generally performs better than $\\epsilon$-greedy action selection, except in the first $k$ steps, when it selects randomly among the as-yet-untried actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "N_timesteps = 1000\n",
    "N_bandits = 1000\n",
    "\n",
    "agent_info_1 = {\"num_actions\": k, \"epsilon\": 0.1}\n",
    "agent_info_2 = {\"num_actions\": k, \"ucb_c\":2}\n",
    "\n",
    "agent_infos = dict(zip([\"eps=0.1\",\"UCB c=2\"],[agent_info_1, agent_info_2]))\n",
    "\n",
    "env = env_bandit.BanditEnvironment    \n",
    "env_info = {\"N\": k}\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in agent_infos}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in agent_infos}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  for k,agent_info in agent_infos.items():\n",
    "\n",
    "    agent = agent_greedy.EpsGreedyAgent\n",
    "    \n",
    "    rl_glue = RLGlue(env, agent)          \n",
    "    rl_glue.rl_init(agent_info, env_info) \n",
    "    rl_glue.rl_start()                    \n",
    "\n",
    "    best_action = list(rl_glue.environment.arms).index(max(rl_glue.environment.arms))\n",
    "    for t in range(N_timesteps):\n",
    "      reward, _, action, _ = rl_glue.rl_step()\n",
    "      results[k][t] += reward / N_bandits\n",
    "      optimal_actions[k][t] += (action==best_action) / N_bandits\n",
    "    \n",
    "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,1)\n",
    "for agent_info in agent_infos:\n",
    "  plt.plot(results[agent_info])\n",
    "plt.legend(agent_infos.keys())\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for agent_info in agent_infos:\n",
    "  plt.plot(optimal_actions[agent_info])\n",
    "plt.legend(agent_infos.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.5\n",
    "Average performance of the gradient bandit algorithm with and without a reward baseline on the 10-armed testbed when the $q_*(a)$ are chosen to be near $+4$ rather than near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "N_timesteps = 1000\n",
    "N_bandits = 1000\n",
    "\n",
    "agent_info_1 = {\"num_actions\": k, \"step_size\": 0.1, \"baseline\": False}\n",
    "agent_info_2 = {\"num_actions\": k, \"step_size\": 0.1, \"baseline\": True}\n",
    "agent_info_3 = {\"num_actions\": k, \"step_size\": 0.4, \"baseline\": False}\n",
    "agent_info_4 = {\"num_actions\": k, \"step_size\": 0.4, \"baseline\": True}\n",
    "\n",
    "agent_infos = dict(zip([r\"Without baseline, $\\alpha=0.1$\",r\"With baseline, $\\alpha=0.1$\",\n",
    "                       r\"Without baseline, $\\alpha=0.4$\",r\"With baseline, $\\alpha=0.4$\"],\n",
    "                       [agent_info_1, agent_info_2,agent_info_3, agent_info_4]))\n",
    "\n",
    "env = env_bandit.BanditEnvironment    \n",
    "env_info = {\"N\": k, \"offset\": 4}\n",
    "\n",
    "results         = {i:[0]*N_timesteps for i in agent_infos}\n",
    "optimal_actions = {i:[0]*N_timesteps for i in agent_infos}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  for k,agent_info in agent_infos.items():\n",
    "\n",
    "    agent = agent_gradient.GradientAgent\n",
    "    \n",
    "    rl_glue = RLGlue(env, agent)          \n",
    "    rl_glue.rl_init(agent_info, env_info) \n",
    "    rl_glue.rl_start()                    \n",
    "\n",
    "    best_action = list(rl_glue.environment.arms).index(max(rl_glue.environment.arms))\n",
    "    for t in range(N_timesteps):\n",
    "      reward, _, action, _ = rl_glue.rl_step()\n",
    "      results[k][t] += reward / N_bandits\n",
    "      optimal_actions[k][t] += (action==best_action) / N_bandits\n",
    "    \n",
    "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,1)\n",
    "for agent_info in agent_infos:\n",
    "  plt.plot(results[agent_info])\n",
    "plt.legend(agent_infos.keys())\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for agent_info in agent_infos:\n",
    "  plt.plot(optimal_actions[agent_info])\n",
    "plt.legend(agent_infos.keys())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2.6\n",
    "A parameter study of the various bandit algorithms presented in this chapter. Each point is the average reward obtained over 1000 steps with a particular algorithm at a particular setting of its parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k = 10\n",
    "\n",
    "N_timesteps = 1000\n",
    "N_bandits = 1000\n",
    "\n",
    "agent_infos = {}\n",
    "for eps in np.power(2.,np.arange(-7,-1,.5)):\n",
    "    key = (\"$\\epsilon$-greedy\",eps)\n",
    "    value = {\"num_actions\": k,\"epsilon\": eps}\n",
    "    agent_infos[key] = value\n",
    "for ucb_c in np.power(2.,np.arange(-4,3,.5)):\n",
    "    key = (\"UCB\",ucb_c)\n",
    "    value = {\"num_actions\": k,\"ucb_c\": ucb_c}\n",
    "    agent_infos[key] = value\n",
    "for val in np.power(2.,np.arange(-2,3,.5)):\n",
    "    key = (\"Optimistic\",val)\n",
    "    value = {\"num_actions\": k,\"initial_value\": val,\"step_size\":0.1}\n",
    "    agent_infos[key] = value\n",
    "for alpha in np.power(2.,np.arange(-5,3,.5)):\n",
    "    key = (\"Gradient\",alpha)\n",
    "    value = {\"num_actions\": k,\"step_size\": alpha}\n",
    "    agent_infos[key] = value\n",
    "    \n",
    "env = env_bandit.BanditEnvironment    \n",
    "env_info = {\"N\": k}\n",
    "\n",
    "results         = {i:0 for i in agent_infos}\n",
    "optimal_actions = {i:0 for i in agent_infos}\n",
    "for bandit in tqdm(range(N_bandits)):\n",
    "  for key,agent_info in agent_infos.items():\n",
    "    if key[0]==\"Gradient\":\n",
    "        agent = agent_gradient.GradientAgent\n",
    "    else:\n",
    "        agent = agent_greedy.EpsGreedyAgent    \n",
    "    rl_glue = RLGlue(env, agent)          \n",
    "    rl_glue.rl_init(agent_info, env_info) \n",
    "    rl_glue.rl_start()                    \n",
    "    for t in range(N_timesteps):\n",
    "      reward, _, _, _ = rl_glue.rl_step()\n",
    "      results[key] += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "values_algo = [a for (a,b) in results.keys()]\n",
    "values_x    = [b for (a,b) in results.keys()]\n",
    "values_y    = [v/bandit/N_timesteps for v in results.values()]\n",
    "df = pd.DataFrame({\"x\":values_x,\"y\":values_y,\"algo\":values_algo})\n",
    "\n",
    "cols = {\"$\\epsilon$-greedy\":\"r\",\"UCB\":\"b\",\"Optimistic\":\"k\",\"Gradient\":\"g\"}\n",
    "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "for a in list(set(df.algo)):\n",
    "    d = df[df[\"algo\"]==a]\n",
    "    plt.semilogx(d.x,d.y,cols[a])\n",
    "plt.legend(list(set(df.algo)))\n",
    "\n",
    "exps = np.arange(-7,3)\n",
    "xticks = np.power(2.,exps)\n",
    "labels = [r\"$2^{\"+str(e)+\"}$\" for e in exps]\n",
    "\n",
    "plt.xticks(xticks,labels)\n",
    "plt.ylim(1,1.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
